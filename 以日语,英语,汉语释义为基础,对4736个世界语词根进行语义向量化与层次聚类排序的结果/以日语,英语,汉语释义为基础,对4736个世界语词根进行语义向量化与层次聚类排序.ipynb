{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/anaconda3/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2025-03-01 20:13:49.170618: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-01 20:13:49.206894: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-01 20:13:50.104520: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 行を処理しました。\n",
      "200 行を処理しました。\n",
      "300 行を処理しました。\n",
      "400 行を処理しました。\n",
      "500 行を処理しました。\n",
      "600 行を処理しました。\n",
      "700 行を処理しました。\n",
      "800 行を処理しました。\n",
      "900 行を処理しました。\n",
      "1000 行を処理しました。\n",
      "1100 行を処理しました。\n",
      "1200 行を処理しました。\n",
      "1300 行を処理しました。\n",
      "1400 行を処理しました。\n",
      "1500 行を処理しました。\n",
      "1600 行を処理しました。\n",
      "1700 行を処理しました。\n",
      "1800 行を処理しました。\n",
      "1900 行を処理しました。\n",
      "2000 行を処理しました。\n",
      "2100 行を処理しました。\n",
      "2200 行を処理しました。\n",
      "2300 行を処理しました。\n",
      "2400 行を処理しました。\n",
      "2500 行を処理しました。\n",
      "2600 行を処理しました。\n",
      "2700 行を処理しました。\n",
      "2800 行を処理しました。\n",
      "2900 行を処理しました。\n",
      "3000 行を処理しました。\n",
      "3100 行を処理しました。\n",
      "3200 行を処理しました。\n",
      "3300 行を処理しました。\n",
      "3400 行を処理しました。\n",
      "3500 行を処理しました。\n",
      "3600 行を処理しました。\n",
      "3700 行を処理しました。\n",
      "3800 行を処理しました。\n",
      "3900 行を処理しました。\n",
      "4000 行を処理しました。\n",
      "4100 行を処理しました。\n",
      "4200 行を処理しました。\n",
      "4300 行を処理しました。\n",
      "4400 行を処理しました。\n",
      "4500 行を処理しました。\n",
      "4600 行を処理しました。\n",
      "4700 行を処理しました。\n",
      "処理完了: 合計 4736 行。出力: 世界语词根的平均语义向量(average_embeddings).csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def main():\n",
    "    input_csv = \"日语,英语,汉语的对应翻译列表.csv\"            # 入力CSVファイル\n",
    "    output_csv = \"世界语词根的平均语义向量(average_embeddings).csv\"   # 出力先のCSVファイル\n",
    "\n",
    "    # 多言語 SentenceTransformer (LaBSE) をロード\n",
    "    model = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "\n",
    "    with open(input_csv, mode='r', encoding='utf-8') as f_in, \\\n",
    "         open(output_csv, mode='w', newline='', encoding='utf-8') as f_out:\n",
    "\n",
    "        reader = csv.reader(f_in)\n",
    "        writer = csv.writer(f_out)\n",
    "\n",
    "        line_count = 0\n",
    "        for row in reader:\n",
    "            # CSVの行が空の場合はスキップ\n",
    "            if not row:\n",
    "                continue\n",
    "\n",
    "            # 3列 (ja, en, zh) を取り出し。列が足りない場合は空文字にする。\n",
    "            ja = row[0].strip() if len(row) > 0 else \"\"\n",
    "            en = row[1].strip() if len(row) > 1 else \"\"\n",
    "            zh = row[2].strip() if len(row) > 2 else \"\"\n",
    "\n",
    "            # 欠損していない言語だけをテキストとしてリストに追加\n",
    "            texts = []\n",
    "            if ja:\n",
    "                texts.append(ja)\n",
    "            if en:\n",
    "                texts.append(en)\n",
    "            if zh:\n",
    "                texts.append(zh)\n",
    "\n",
    "            if not texts:\n",
    "                # すべて空欄(日本語・英語・中国語が欠損)の場合はゼロベクトルを出力\n",
    "                zero_vec = np.zeros(768, dtype=float)\n",
    "                writer.writerow(zero_vec.tolist())\n",
    "            else:\n",
    "                # エンコード(複数テキスト -> (#texts, 768))\n",
    "                embeddings = model.encode(texts, convert_to_numpy=True)\n",
    "                # 平均ベクトル (shape: (768,))\n",
    "                avg_embedding = np.mean(embeddings, axis=0)\n",
    "                # CSVに書き込み\n",
    "                writer.writerow(avg_embedding.tolist())\n",
    "\n",
    "            line_count += 1\n",
    "            if line_count % 100 == 0:\n",
    "                print(f\"{line_count} 行を処理しました。\")\n",
    "\n",
    "    print(f\"処理完了: 合計 {line_count} 行。出力: {output_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "読み込んだベクトルの行数: 4736, 次元: 768\n",
      "行間のコサイン距離を計算しています...\n",
      "階層クラスタリングを実行しています...\n",
      "デンドログラム葉順を取得...\n",
      "leaf_order.txt に葉順を保存しています...\n",
      "基于语义向量进行层次聚类的世界语词根列表.csv に並べ替え結果を出力します...\n",
      "完了しました！\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "\n",
    "def main():\n",
    "    # 前段で作成したファイル（平均ベクトル）\n",
    "    embeddings_csv = \"世界语词根的平均语义向量(average_embeddings).csv\"\n",
    "    # 元のテキストデータ\n",
    "    original_csv = \"世界语词根列表(4736个).csv\"\n",
    "    # 並べ替え後のテキストを保存する先\n",
    "    reordered_csv = \"基于语义向量进行层次聚类的世界语词根列表.csv\"\n",
    "\n",
    "    # 1) 埋め込みベクトルの読み込み\n",
    "    embeddings = []\n",
    "    with open(embeddings_csv, mode='r', encoding='utf-8') as f_emb:\n",
    "        reader = csv.reader(f_emb)\n",
    "        for row in reader:\n",
    "            vec = list(map(float, row))  # row を float に変換\n",
    "            embeddings.append(vec)\n",
    "    embeddings = np.array(embeddings)  # shape = (N, 768)\n",
    "    n_rows = embeddings.shape[0]\n",
    "    print(f\"読み込んだベクトルの行数: {n_rows}, 次元: {embeddings.shape[1]}\")\n",
    "\n",
    "    # 2) 元のテキストを行単位で読み込み\n",
    "    original_lines = []\n",
    "    with open(original_csv, mode='r', encoding='utf-8') as f_in:\n",
    "        reader = csv.reader(f_in)\n",
    "        for row in reader:\n",
    "            original_lines.append(row)\n",
    "\n",
    "    # 行数チェック\n",
    "    if len(original_lines) != n_rows:\n",
    "        print(\"警告: input_data.csv と average_embeddings.csv の行数が一致しません。\")\n",
    "        print(f\"  テキスト行数: {len(original_lines)}, ベクトル行数: {n_rows}\")\n",
    "        # 必要に応じて処理を終了させるか、短い方に合わせるなどの対応が必要\n",
    "\n",
    "    # 3) 距離行列を計算（コサイン距離）\n",
    "    print(\"行間のコサイン距離を計算しています...\")\n",
    "    dist_array = pdist(embeddings, metric='cosine')\n",
    "\n",
    "    # 4) 階層クラスタリング\n",
    "    print(\"階層クラスタリングを実行しています...\")\n",
    "    # complete法（完全連結法）例。'average', 'single' なども使用可。\n",
    "    Z = linkage(dist_array, method='complete')  \n",
    "\n",
    "    # 5) デンドログラムの葉順を取得\n",
    "    print(\"デンドログラム葉順を取得...\")\n",
    "    leaf_order = leaves_list(Z)  # shape (N,)\n",
    "\n",
    "    # ここで leaf_order をファイルに書き出す (テキストファイル)\n",
    "    print(\"leaf_order.txt に葉順を保存しています...\")\n",
    "    np.savetxt(\"leaf_order.txt\", leaf_order, fmt='%d')\n",
    "\n",
    "    # 6) 得られた順序に従って original_lines を並べ替え\n",
    "    reordered_lines = [original_lines[idx] for idx in leaf_order]\n",
    "\n",
    "    # 7) 並べ替え後の結果を出力\n",
    "    print(f\"{reordered_csv} に並べ替え結果を出力します...\")\n",
    "    with open(reordered_csv, mode='w', newline='', encoding='utf-8') as f_out:\n",
    "        writer = csv.writer(f_out)\n",
    "        for row in reordered_lines:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(\"完了しました！\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完了: 用于交由生成AI决定汉字分配的世界语词根及其日语,英语,汉语对应翻译列表.csv を 基于世界语词根的平均语义向量进行层次聚类排序后的,用于交由生成AI决定汉字分配的世界语词根及其日语,英语,汉语对应翻译列表.csv に並び替え出力しました。\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "def reorder_csv(input_csv, output_csv, leaf_order_file=\"leaf_order.txt\"):\n",
    "    # 並び順を読み込み\n",
    "    leaf_order = np.loadtxt(leaf_order_file, dtype=int)\n",
    "\n",
    "    # 入力CSVを行単位で読み込み\n",
    "    lines = []\n",
    "    with open(input_csv, mode='r', encoding='utf-8') as f_in:\n",
    "        reader = csv.reader(f_in)\n",
    "        for row in reader:\n",
    "            lines.append(row)\n",
    "\n",
    "    # 行数が leaf_order と一致するか確認\n",
    "    n_lines = len(lines)\n",
    "    if n_lines != len(leaf_order):\n",
    "        print(\"行数が一致しません。並び替え不可。\")\n",
    "        return\n",
    "\n",
    "    # 並び替えを適用\n",
    "    reordered_lines = [lines[idx] for idx in leaf_order]\n",
    "\n",
    "    # 結果を書き出し\n",
    "    with open(output_csv, mode='w', newline='', encoding='utf-8') as f_out:\n",
    "        writer = csv.writer(f_out)\n",
    "        writer.writerows(reordered_lines)\n",
    "\n",
    "    print(f\"完了: {input_csv} を {output_csv} に並び替え出力しました。\")\n",
    "\n",
    "# 使い方:\n",
    "reorder_csv(\"用于交由生成AI决定汉字分配的世界语词根及其日语,英语,汉语对应翻译列表.csv\", \"基于世界语词根的平均语义向量进行层次聚类排序后的,用于交由生成AI决定汉字分配的世界语词根及其日语,英语,汉语对应翻译列表.csv\", leaf_order_file=\"leaf_order.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
